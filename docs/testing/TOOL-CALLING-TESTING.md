# Tool Calling æµ‹è¯•æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•æµ‹è¯• PonyBunny çš„åŸç”Ÿ tool calling åŠŸèƒ½ã€‚

## æµ‹è¯•å±‚çº§

### 1. å•å…ƒæµ‹è¯• - Protocol Adapters

æµ‹è¯•åè®®é€‚é…å™¨æ˜¯å¦æ­£ç¡®è½¬æ¢å·¥å…·è°ƒç”¨æ ¼å¼ã€‚

```bash
# è¿è¡Œ protocol adapter æµ‹è¯•
npx jest test/infra/llm/protocols/tool-calling.test.ts
```

**æµ‹è¯•å†…å®¹ï¼š**
- âœ… å·¥å…·å®šä¹‰æ ¼å¼è½¬æ¢ï¼ˆç»Ÿä¸€æ ¼å¼ â†’ provider æ ¼å¼ï¼‰
- âœ… å·¥å…·è°ƒç”¨æ¶ˆæ¯æ ¼å¼åŒ–
- âœ… å·¥å…·ç»“æœæ¶ˆæ¯æ ¼å¼åŒ–
- âœ… å“åº”è§£æï¼ˆprovider æ ¼å¼ â†’ ç»Ÿä¸€æ ¼å¼ï¼‰
- âœ… Thinking å†…å®¹æå–
- âœ… Streaming chunk è§£æ

### 2. E2E æµ‹è¯• - å®Œæ•´æµç¨‹

æµ‹è¯•å®Œæ•´çš„å·¥å…·è°ƒç”¨æµç¨‹ï¼ŒåŒ…æ‹¬ LLM è°ƒç”¨å’Œå·¥å…·æ‰§è¡Œã€‚

```bash
# è¿è¡Œ E2E demoï¼ˆéœ€è¦é…ç½® API keysï¼‰
npx tsx test/e2e/tool-calling-demo.ts
```

**æµ‹è¯•åœºæ™¯ï¼š**
1. **ç®€å•å¯¹è¯**ï¼šä¸ä½¿ç”¨å·¥å…·çš„åŸºç¡€å¯¹è¯
2. **å•æ¬¡å·¥å…·è°ƒç”¨**ï¼šLLM è¯·æ±‚è°ƒç”¨å·¥å…·
3. **å¤šè½®å·¥å…·è°ƒç”¨**ï¼šå·¥å…·æ‰§è¡Œåç»§ç»­å¯¹è¯
4. **Thinking æ¨¡å¼**ï¼šæµ‹è¯•æ¨ç†è¿‡ç¨‹è®°å½•

### 3. é›†æˆæµ‹è¯• - ReAct Integration

æµ‹è¯• ReAct å¾ªç¯ä¸­çš„åŸç”Ÿå·¥å…·è°ƒç”¨ã€‚

```bash
# åˆ›å»ºæµ‹è¯•æ–‡ä»¶
npx tsx test/integration/react-tool-calling.test.ts
```

## å‰ç½®å‡†å¤‡

### 1. é…ç½® API Keys

ç¡®ä¿å·²é…ç½®è‡³å°‘ä¸€ä¸ª LLM provider çš„ API keyï¼š

```bash
# ç¼–è¾‘ credentials.json
vim ~/.ponybunny/credentials.json
```

ç¤ºä¾‹é…ç½®ï¼š

```json
{
  "$schema": "./credentials.schema.json",
  "endpoints": {
    "anthropic-direct": {
      "enabled": true,
      "apiKey": "sk-ant-xxx"
    },
    "openai-direct": {
      "enabled": true,
      "apiKey": "sk-xxx"
    }
  }
}
```

### 2. é…ç½® LLM Models

ç¡®ä¿ `llm-config.json` ä¸­é…ç½®äº†æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ï¼š

```json
{
  "models": {
    "claude-3-5-sonnet-20241022": {
      "displayName": "Claude 3.5 Sonnet",
      "costPer1kInput": 0.003,
      "costPer1kOutput": 0.015,
      "contextWindow": 200000,
      "thinking": true,
      "streaming": true,
      "endpoints": ["anthropic-direct"]
    },
    "gpt-4-turbo": {
      "displayName": "GPT-4 Turbo",
      "costPer1kInput": 0.01,
      "costPer1kOutput": 0.03,
      "contextWindow": 128000,
      "streaming": true,
      "endpoints": ["openai-direct"]
    }
  }
}
```

## æµ‹è¯•æ­¥éª¤

### Step 1: è¿è¡Œå•å…ƒæµ‹è¯•

```bash
# ç¼–è¯‘é¡¹ç›®
npm run build

# è¿è¡Œ protocol adapter æµ‹è¯•
npx jest test/infra/llm/protocols/tool-calling.test.ts --verbose
```

**é¢„æœŸç»“æœï¼š**
- âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡
- âœ… å·¥å…·å®šä¹‰æ ¼å¼æ­£ç¡®
- âœ… æ¶ˆæ¯è½¬æ¢æ­£ç¡®
- âœ… å“åº”è§£ææ­£ç¡®

### Step 2: è¿è¡Œ E2E Demo

```bash
# è¿è¡Œå®Œæ•´çš„ E2E demo
npx tsx test/e2e/tool-calling-demo.ts
```

**é¢„æœŸè¾“å‡ºï¼š**

```
ğŸš€ Tool Calling Demo

ğŸ“¦ Available tools: web_search, find_skills

=== Test 1: Simple Conversation ===
Response: Hello! I'm doing well, thank you for asking...
Tokens used: 45
Finish reason: stop

=== Test 2: Tool Calling (Web Search) ===
Response content: null
Tokens used: 120
Finish reason: tool_calls

ğŸ”§ Tool calls detected:
  - web_search
    Arguments: {"query":"weather in Shanghai today"}

=== Test 3: Multi-turn with Tool Execution ===
First response:
  Content: null
  Finish reason: tool_calls

ğŸ”§ Tool calls:
  - Executing web_search...

ğŸ“¥ Sending tool results back to LLM...

Final response:
  Content: Based on the search results, here's what I found...
  Finish reason: stop

=== Test 4: Thinking Mode ===
Response: Recursion is a programming technique where...
Tokens used: 250

ğŸ’­ Thinking process:
Let me break down the concept of recursion step by step...

âœ… Demo completed!
```

### Step 3: æµ‹è¯•ä¸åŒ Provider

ä¿®æ”¹ `llm-config.json` ä¸­çš„ agent é…ç½®æ¥æµ‹è¯•ä¸åŒ providerï¼š

```json
{
  "agents": {
    "conversation": {
      "tier": "medium",
      "models": ["claude-3-5-sonnet-20241022"]  // æˆ– "gpt-4-turbo"
    }
  }
}
```

ç„¶åé‡æ–°è¿è¡Œ E2E demoã€‚

### Step 4: æµ‹è¯• Streaming

åˆ›å»º streaming æµ‹è¯•ï¼š

```typescript
// test/e2e/streaming-tool-calling.ts
import { getLLMService } from '../../src/infra/llm/llm-service.js';
import { getGlobalToolProvider } from '../../src/infra/tools/tool-provider.js';

async function testStreaming() {
  const llmService = getLLMService();
  const toolProvider = getGlobalToolProvider();
  const tools = toolProvider.getToolDefinitions();

  const messages = [
    { role: 'system', content: 'You are a helpful assistant.' },
    { role: 'user', content: 'Tell me a story about AI' },
  ];

  console.log('Testing streaming...\n');

  const response = await llmService.completeForAgent('conversation', messages, {
    maxTokens: 500,
    stream: true,
    thinking: true,
    onChunk: (chunk) => {
      if (chunk.thinking) {
        process.stdout.write(`[THINKING] ${chunk.thinking}`);
      }
      if (chunk.content) {
        process.stdout.write(chunk.content);
      }
      if (chunk.done) {
        console.log(`\n\n[DONE] Finish reason: ${chunk.finishReason}`);
      }
    },
  });

  console.log('\nFinal response:', response.content);
}

testStreaming().catch(console.error);
```

è¿è¡Œï¼š

```bash
npx tsx test/e2e/streaming-tool-calling.ts
```

## éªŒè¯æ¸…å•

### âœ… æ ¸å¿ƒåŠŸèƒ½

- [ ] å·¥å…·å®šä¹‰æ­£ç¡®ç”Ÿæˆï¼ˆJSON Schema æ ¼å¼ï¼‰
- [ ] å·¥å…·è°ƒç”¨æ¶ˆæ¯æ­£ç¡®æ ¼å¼åŒ–
- [ ] å·¥å…·ç»“æœæ¶ˆæ¯æ­£ç¡®æ ¼å¼åŒ–
- [ ] LLM èƒ½å¤Ÿè¯·æ±‚å·¥å…·è°ƒç”¨
- [ ] å·¥å…·è°ƒç”¨å‚æ•°æ­£ç¡®è§£æ
- [ ] å·¥å…·æ‰§è¡Œç»“æœæ­£ç¡®è¿”å›
- [ ] å¤šè½®å·¥å…·è°ƒç”¨æ­£å¸¸å·¥ä½œ

### âœ… Provider æ”¯æŒ

- [ ] Anthropic: å·¥å…·è°ƒç”¨æ­£å¸¸
- [ ] Anthropic: Thinking æ¨¡å¼æ­£å¸¸
- [ ] Anthropic: Streaming æ­£å¸¸
- [ ] OpenAI: å·¥å…·è°ƒç”¨æ­£å¸¸
- [ ] OpenAI: Reasoning content (o1) æ­£å¸¸
- [ ] OpenAI: Streaming æ­£å¸¸
- [ ] Gemini: å·¥å…·è°ƒç”¨æ­£å¸¸
- [ ] Gemini: Streaming æ­£å¸¸

### âœ… é”™è¯¯å¤„ç†

- [ ] å·¥å…·ä¸å­˜åœ¨æ—¶æ­£ç¡®å¤„ç†
- [ ] å·¥å…·å‚æ•°é”™è¯¯æ—¶æ­£ç¡®å¤„ç†
- [ ] å·¥å…·æ‰§è¡Œå¤±è´¥æ—¶æ­£ç¡®å¤„ç†
- [ ] API é”™è¯¯æ—¶æ­£ç¡® fallback
- [ ] Streaming ä¸­æ–­æ—¶æ­£ç¡®æ¢å¤

### âœ… æ€§èƒ½

- [ ] å“åº”æ—¶é—´ < 3 ç§’ï¼ˆå•æ¬¡å·¥å…·è°ƒç”¨ï¼‰
- [ ] å†…å­˜ä½¿ç”¨æ­£å¸¸
- [ ] æ— å†…å­˜æ³„æ¼
- [ ] Streaming å»¶è¿Ÿä½

## å¸¸è§é—®é¢˜

### Q1: æµ‹è¯•æ—¶ LLM ä¸è°ƒç”¨å·¥å…·ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. æ¨¡å‹ä¸æ”¯æŒå·¥å…·è°ƒç”¨
2. å·¥å…·å®šä¹‰ä¸æ¸…æ™°
3. ç”¨æˆ·æç¤ºä¸æ˜ç¡®

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨æ”¯æŒå·¥å…·è°ƒç”¨çš„æ¨¡å‹ï¼ˆClaude 3.5, GPT-4, Gemini Proï¼‰
- æ”¹è¿›å·¥å…·æè¿°å’Œå‚æ•°è¯´æ˜
- ä½¿ç”¨æ›´æ˜ç¡®çš„æç¤ºè¯ï¼ˆå¦‚ "Search for..."ï¼‰

### Q2: å·¥å…·è°ƒç”¨å‚æ•°è§£æå¤±è´¥ï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. JSON Schema å®šä¹‰ä¸æ­£ç¡®
2. LLM è¿”å›çš„å‚æ•°æ ¼å¼ä¸ç¬¦åˆé¢„æœŸ

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥ `tool-provider.ts` ä¸­çš„å‚æ•°å®šä¹‰
- æ·»åŠ å‚æ•°éªŒè¯é€»è¾‘
- æŸ¥çœ‹ LLM è¿”å›çš„åŸå§‹å‚æ•°

### Q3: Streaming æ¨¡å¼ä¸‹å·¥å…·è°ƒç”¨ä¸å·¥ä½œï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. Streaming ä¸­å·¥å…·è°ƒç”¨éœ€è¦ç´¯ç§¯å¤šä¸ª chunk
2. å½“å‰å®ç°ç®€åŒ–äº† streaming å·¥å…·è°ƒç”¨

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨é streaming æ¨¡å¼æµ‹è¯•å·¥å…·è°ƒç”¨
- æˆ–å®ç°å®Œæ•´çš„ streaming å·¥å…·è°ƒç”¨çŠ¶æ€ç®¡ç†

### Q4: Thinking å†…å®¹ä¸ºç©ºï¼Ÿ

**å¯èƒ½åŸå› ï¼š**
1. æ¨¡å‹ä¸æ”¯æŒ thinking æ¨¡å¼
2. é…ç½®ä¸­æœªå¯ç”¨ thinking

**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨æ”¯æŒ thinking çš„æ¨¡å‹ï¼ˆClaude 3.5 with extended thinking, OpenAI o1ï¼‰
- åœ¨ `llm-config.json` ä¸­è®¾ç½® `"thinking": true`
- åœ¨è°ƒç”¨æ—¶ä¼ é€’ `thinking: true` é€‰é¡¹

## ä¸‹ä¸€æ­¥

å®Œæˆæµ‹è¯•åï¼Œå¯ä»¥ï¼š

1. **é›†æˆåˆ° CI/CD**ï¼šæ·»åŠ è‡ªåŠ¨åŒ–æµ‹è¯•
2. **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆ†æå’Œä¼˜åŒ–å·¥å…·è°ƒç”¨æ€§èƒ½
3. **æ‰©å±•å·¥å…·**ï¼šæ·»åŠ æ›´å¤šå·¥å…·å®šä¹‰
4. **æ”¹è¿› UX**ï¼šåœ¨ UI ä¸­æ˜¾ç¤ºå·¥å…·è°ƒç”¨è¿‡ç¨‹

## å‚è€ƒæ–‡æ¡£

- [Plan: Native Tool Calling](.claude/plan.md)
- [Anthropic Tool Use API](https://docs.anthropic.com/claude/docs/tool-use)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [Gemini Function Calling](https://ai.google.dev/docs/function_calling)
