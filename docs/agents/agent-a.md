# PonyBunny — Agent A Skills (Final)

**Agent Name:** `agent_a_market_listener`
**Purpose:** 24x7 passive market listening: **read → detect → extract → tag → store**
**Mode:** **Local-first**, Dockerized MCP servers, **Postgres** storage, **Playwright** for tough pages
**Transport Assumption:** MCP over **HTTP/SSE** (container endpoints). If your PonyBunny uses stdio, the tool allowlist stays the same—only transport wiring changes.

---

## 0) Hard Guardrails (Non-negotiable)

### Agent A must NEVER:

* Post/reply/DM anywhere
* Take payments or contact users
* Execute arbitrary OS commands
* Generate “advice” or “solutions”
* Make commercial decisions (that’s Agent B)

### Allowed behaviour:

* Read public data (or via official APIs)
* Extract *original* problem statements (no paraphrase)
* Store append-only records
* Maintain checkpoint/cursor and dedupe keys

---

## 1) Runtime Topology (Local + Docker)

### Required Docker services (MCP endpoints)

* **Playwright MCP**: `http://localhost:7777`
* **Reddit MCP**: `http://localhost:3000`
* **GitHub MCP**: `http://localhost:4000`
* **Postgres MCP**: `http://localhost:8000`
* **Postgres DB**: `postgresql://pony:pony_pass@localhost:5432/ponybunny`

> Agent A (PonyBunny runtime) runs on your local machine and calls these MCP servers.

---

## 2) Canonical Pipeline (Strict Call Order)

**One tick = one deterministic pipeline run**:

1. `control.tick`
2. For each configured source:

   * `source.read_stream`
   * For each item:

     * `text.detect_problem_signal`
     * If true:

       * `text.extract_problem_block`
       * `analysis.guess_author_role`
       * `data.store_record`

No other order is valid.

---

## 3) Call Graph

```mermaid
flowchart TD
  T[control.tick] --> S[source.read_stream]
  S --> I{for each item}
  I --> D[text.detect_problem_signal]
  D -->|false| K[skip]
  D -->|true| E[text.extract_problem_block]
  E --> R[analysis.guess_author_role]
  R --> W[data.store_record]
  W --> I
```

---

## 4) Tool Allowlist (Global)

Agent A can only call tools in this allowlist.

### LLM tools (internal)

* `llm.extract_json`
* `llm.classify`

### MCP tools (HTTP/SSE)

* **Playwright MCP**

  * `playwright.navigate`
  * `playwright.get_content`
  * `playwright.query_selector_all`
* **Reddit MCP**

  * `reddit.list_new_posts`
  * `reddit.list_new_comments`
* **GitHub MCP**

  * `github.list_issues`
  * `github.list_issue_comments`
* **Postgres MCP**

  * `pg.insert`
  * `pg.select`
  * `pg.execute` *(restricted to approved SQL only; see below)*

### Forbidden tools (explicit deny)

* Any `*.post`, `*.reply`, `*.send_message`, `*.create_*`, `*.update_*`, `*.delete_*`
* Any OS/terminal execution tool
* Any payment/billing tool
* Any “web scrape bypass” / stealth / fingerprint spoof tools

---

## 5) Skills Specification

> **Format**: skills.sh-style doc sections: **Intent → Inputs → Outputs → Tools → Policy → Failure modes**

---

### Skill A0 — `control.tick` (Deterministic Orchestrator Entry)

**Intent**
Runs one scheduled cycle. Handles: source selection, rate limiting, checkpoint read/write, retries, metrics.

**Inputs**

```yaml
tick:
  run_id: string           # generated by orchestrator
  now: iso8601             # current time
  max_sources_per_tick: 10
  max_items_per_source: 50
  default_time_window: "6h"
```

**Outputs**

```yaml
tick_result:
  run_id: string
  sources_processed: int
  items_fetched: int
  items_scanned: int
  items_stored: int
  errors: int
  duration_ms: int
```

**Allowed Tools**

* `pg.select` (read source configs, checkpoints)
* `pg.execute` (approved statements only)
* `pg.insert` (run logs)
* (Optional) in-memory scheduler / PonyBunny internal timer — not MCP

**Policy**

* **No LLM calls** here
* Must apply per-platform rate limits + exponential backoff
* Must update checkpoints *after* successful processing
* Must hard-stop a source via circuit breaker after repeated failures

**Failure Modes**

* If a source fails: mark failure, apply backoff, continue other sources
* If DB is down: fail the tick (do not process)

---

### Skill A1 — `source.read_stream` (Multi-source Read)

**Intent**
Fetches new items from a configured source using official APIs where possible; otherwise browser read via Playwright.

**Inputs**

```yaml
source_request:
  platform: enum [reddit, github, forum_web]
  source_id: string            # subreddit, repo "org/name", or URL
  cursor: string|null          # checkpoint cursor
  time_window: duration        # e.g. "6h"
  max_items: int
```

**Outputs**

```yaml
raw_items:
  - platform: string
    source_id: string
    permalink: string
    author: string|null
    created_at: iso8601|null
    raw_text: string
    raw_html: string|null
    metadata: object
```

**Allowed Tools (by platform)**

**Reddit**

* `reddit.list_new_posts({subreddit, after, limit})`
* `reddit.list_new_comments({subreddit, after, limit})`

**GitHub**

* `github.list_issues({repo, since, state})`
* `github.list_issue_comments({repo, issue_number, since})`

**Forum/Web (Playwright)**

* `playwright.navigate({url})`
* `playwright.get_content({mode: "text"})`
* `playwright.query_selector_all({selector})` *(optional for thread lists)*

**Policy**

* Must respect `robots.txt` and site terms where applicable
* No login escalation
* No attempt to bypass anti-bot beyond normal browser access

**Failure Modes**

* If rate limited: return error + suggested backoff seconds
* If content too large: truncate raw_text at configured cap (store hash + cap flag)

---

### Skill A2 — `text.detect_problem_signal` (Problem Signal Detection)

**Intent**
Decides whether a raw item contains a user-expressed problem/pain/need.

**Inputs**

```yaml
detect_request:
  raw_text: string
  platform: string
```

**Outputs**

```yaml
detect_result:
  has_problem_signal: boolean
  signal_markers: string[]     # short quotes/phrases that triggered detection
  label: enum [problem, how_to, bug, request, complaint, discussion, showcase, other]
  confidence: float            # 0..1 (for audit only)
```

**Allowed Tools**

* `llm.classify` *(fast path)*
* OR `llm.extract_json` *(stable path)*

**Policy**

* No scoring, no business judgement
* Must keep markers **verbatim snippets** (<= 20 words each)
* If uncertain: default to **true** when `confidence` in [0.45..0.55] (bias toward capture)

**Failure Modes**

* If LLM fails: fallback rule-based heuristics (keywords) and mark `confidence=0.0`

---

### Skill A3 — `text.extract_problem_block` (Verbatim Block Extraction)

**Intent**
Extract the smallest useful verbatim block that represents the problem, plus minimal surrounding context.

**Inputs**

```yaml
extract_request:
  raw_text: string
  window_chars: 300
  platform: string
```

**Outputs**

```yaml
extract_result:
  problem_raw_text: string         # verbatim, minimal
  surrounding_context: string      # verbatim, small
  mentioned_tools: string[]        # optional
  constraints: string[]            # optional
```

**Allowed Tools**

* `llm.extract_json`

**Policy**

* **No paraphrase** (verbatim only)
* Must not “clean up” grammar
* If raw_text is short, problem_raw_text may equal raw_text

**Failure Modes**

* If extraction fails: set `problem_raw_text = raw_text` and `surrounding_context=""`, mark `extraction_fallback=true`

---

### Skill A4 — `analysis.guess_author_role` (Weak Role Guess)

**Intent**
Heuristic guess of author role. **Never treated as truth**.

**Inputs**

```yaml
role_request:
  raw_text: string
  platform: string
```

**Outputs**

```yaml
role_result:
  role_guess: enum [founder, employee, developer, ops, student, hobbyist, unknown]
  confidence: float                # MUST be clamped <= 0.5
```

**Allowed Tools**

* `llm.classify`

**Policy**

* Clamp confidence: `confidence = min(confidence, 0.5)`
* If uncertain: `unknown, 0.1`

---

### Skill A5 — `data.store_record` (Append-only Storage)

**Intent**
Writes one immutable observation into Postgres, with dedupe and checkpoint updates handled by the orchestrator.

**Inputs**

```yaml
store_request:
  platform: string
  source_id: string
  permalink: string
  author: string|null
  created_at: iso8601|null
  problem_raw_text: string
  surrounding_context: string
  label: string
  signal_markers: string[]
  role_guess: string
  role_confidence: float
  raw_text_hash: string
  ingest_run_id: string
```

**Outputs**

```yaml
store_result:
  stored: boolean
  record_id: string|null
  deduped: boolean
```

**Allowed Tools**

* `pg.select` *(dedupe check by permalink/hash)*
* `pg.insert` *(insert observation)*
* `pg.execute` *(approved statements only)*

**Policy**

* Append-only: never update/delete observations
* Dedupe priority:

  1. `permalink`
  2. `raw_text_hash`
* If deduped: return `stored=false, deduped=true`

---

## 6) Approved SQL (Postgres)

> If you allow `pg.execute`, restrict it to these statements (or equivalent prepared statements).

### Tables

```sql
-- 1) Sources: what to listen to
create table if not exists agent_a_sources (
  id bigserial primary key,
  platform text not null check (platform in ('reddit','github','forum_web')),
  source_id text not null,                 -- subreddit / repo / url
  enabled boolean not null default true,
  poll_interval_seconds int not null default 600,
  max_items int not null default 50,
  priority int not null default 50,         -- lower = earlier
  created_at timestamptz not null default now()
);

create unique index if not exists idx_agent_a_sources_unique
on agent_a_sources(platform, source_id);

-- 2) Checkpoints: per source cursor/watermark
create table if not exists agent_a_checkpoints (
  platform text not null,
  source_id text not null,
  cursor text,
  last_seen_at timestamptz,
  backoff_until timestamptz,
  failure_count int not null default 0,
  updated_at timestamptz not null default now(),
  primary key (platform, source_id)
);

-- 3) Dedupe keys (optional but recommended)
create table if not exists agent_a_dedupe (
  key text primary key,                     -- permalink or raw_hash
  created_at timestamptz not null default now()
);

-- 4) Observations (append-only)
create table if not exists agent_a_observations (
  id bigserial primary key,
  platform text not null,
  source_id text not null,
  permalink text not null,
  author text,
  created_at timestamptz,
  label text not null,
  signal_markers jsonb not null default '[]'::jsonb,
  problem_raw_text text not null,
  surrounding_context text not null default '',
  role_guess text not null default 'unknown',
  role_confidence real not null default 0.1,
  raw_text_hash text not null,
  ingest_run_id text not null,
  inserted_at timestamptz not null default now()
);

create unique index if not exists idx_agent_a_obs_permalink
on agent_a_observations(permalink);

create index if not exists idx_agent_a_obs_hash
on agent_a_observations(raw_text_hash);

-- 5) Runs (metrics/logs)
create table if not exists agent_a_runs (
  run_id text primary key,
  started_at timestamptz not null default now(),
  finished_at timestamptz,
  sources_processed int not null default 0,
  items_fetched int not null default 0,
  items_scanned int not null default 0,
  items_stored int not null default 0,
  errors int not null default 0,
  meta jsonb not null default '{}'::jsonb
);
```

---

## 7) Rate Limit + Backoff + Circuit Breaker (Deterministic)

### Rate limit (suggested defaults)

```yaml
rate_limits:
  reddit:
    max_requests_per_minute: 30
    backoff_on_429_seconds: [60, 120, 300]
  github:
    max_requests_per_minute: 60
    backoff_on_403_seconds: [60, 120, 300]
  forum_web:
    max_requests_per_minute: 10
    backoff_on_403_429_seconds: [120, 300, 900]
```

### Circuit breaker

* If `failure_count >= 5`: set `backoff_until = now + 6h` and reset `failure_count` to 0
* Any success resets `failure_count = 0`

---

## 8) Item Size Caps (Token Safety)

```yaml
limits:
  raw_text_max_chars: 12000
  problem_raw_text_max_chars: 2000
  surrounding_context_max_chars: 2000
  signal_markers_max_items: 5
```

If `raw_text` exceeds cap:

* store `raw_text_hash` and truncated text (or just hash + excerpt)
* set `metadata.truncated=true`

---

## 9) Minimal “Sources” Seed (Examples)

```sql
insert into agent_a_sources(platform, source_id, poll_interval_seconds, max_items, priority)
values
  ('reddit', 'SaaS', 600, 50, 10),
  ('reddit', 'devops', 900, 50, 20),
  ('github', 'someorg/somerepo', 900, 50, 30),
  ('forum_web', 'https://example.com/forum/latest', 1800, 30, 60)
on conflict do nothing;
```

---

## 10) Docker Compose (Reference)

Use the compose you already confirmed (Postgres + Playwright + Reddit MCP + GitHub MCP + Postgres MCP).
**Agent A** runs locally and calls those endpoints.

---

## 11) Compliance / Safety Notes (Practical)

* Prefer official APIs (Reddit/GitHub) over scraping.
* For web forums, Playwright should read public pages only.
* Do not automate replies in Agent A. Keep A “mute” forever.

---

## 12) “Definition of Done” for Agent A

Agent A is considered complete when:

* It runs unattended for 24h without manual intervention
* Checkpoints advance correctly
* Dedupe prevents duplicates
* Observations are append-only and queryable
* Metrics show stable ingest volume and low error rate

---

# Next (You’ll Want This Immediately)

If you want, I’ll follow this with **two ready-to-use files** (same style, no fluff):

1. `agent-a.skills.md` split into **one skill = one file** (skills.sh friendly)
2. `agent-a.orchestrator-policy.yaml` (rate limit/backoff/circuit breaker + tool allowlist in machine-readable form)

Just say: **“Split into per-skill files”** or **“Keep single file”**.
